@inproceedings{ye2023just,
  selected={true},
author = {Ye, Lyumanshan and Yue, Jiatong and Wei, Yiwen and Liang, Shuai and Chang, Danni},
title = {“Just Like Blooming Fireworks, And Match With Function Perfectly”: Explore and Evaluate User-Defined One-Handed Gestures of Smartwatch},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585914},
doi = {10.1145/3544549.3585914},
abstract = {One-handed gesture interaction is a more convenient input interaction method on smartwatches for some special scenarios, e.g. wearing a smartwatch when running or biking. To explore user-friendly one-handed gestures, what users are thinking when using the gesture, and what characteristic would make the user feel this one-handed gesture is friendly, we developed a series of one-handed gestures for 6 basic functions of the smartwatch. The end-user elicitation method resulted in 12 new one-hand gestures. We compared these 12 user-defined one-handed gestures with the Apple Watch one-handed gestures. We developed a Wizard of Oz model and evaluated these gestures by using qualitative and quantitative approaches. The results show that we generated a set of one-handed gestures that are more friendly than the existing Apple Watch one-handed gestures. Also, during the evaluation process, we collected quantitative data and interesting user perspectives. We also gave some design recommendations for one-handed gestures.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {195},
numpages = {9},
keywords = {Gesture generation, One-handed gestures, User-defined gestures},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{ye2024sensebot,
  selected={true},
author = {Ye, Lyumanshan and Jin, Hao and Jin, Qiao},
title = {SenseBot: Leveraging Embodied Asymmetric Interaction and Social Robotic to Enhance Intergenerational Communication},
year = {2024},
isbn = {9798400707186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672539.3686734},
doi = {10.1145/3672539.3686734},
abstract = {Embodied communication, which uses physical cues to convey meaning and emotions, is essential for building social connections. In this work, we introduce SenseBot, an augmented reality (AR)-enabled robotic representative designed to facilitate embodied communication. SenseBot acts as a local agent for remote users, allowing them to engage more interactively and meaningfully with people on-site.},
booktitle = {Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {3},
numpages = {3},
keywords = {AR, Robot, embodied communication, remote communication, social connection},
location = {Pittsburgh, PA, USA},
series = {UIST Adjunct '24}
}

@inproceedings{zhang2021patterns,
  selected={true},
author = {Zhang, Shengchen and Wang, Zixuan and Chen, Chaoran and Dai, Yi and Ye, Lyumanshan and Sun, Xiaohua},
title = {Patterns for Representing Knowledge Graphs to Communicate Situational Knowledge of Service Robots},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445767},
doi = {10.1145/3411764.3445767},
abstract = {Service robots are envisioned to be adaptive to their working environment based on situational knowledge. Recent research focused on designing visual representation of knowledge graphs for expert users. However, how to generate an understandable interface for non-expert users remains to be explored. In this paper, we use knowledge graphs (KGs) as a common ground for knowledge exchange and develop a pattern library for designing KG interfaces for non-expert users. After identifying the types of robotic situational knowledge from the literature, we present a formative study in which participants used cards to communicate the knowledge for given scenarios. We iteratively coded the results and identified patterns for representing various types of situational knowledge. To derive design recommendations for applying the patterns, we prototyped a lab service robot and conducted Wizard-of-Oz testing. The patterns and recommendations could provide useful guidance in designing knowledge-exchange interfaces for robots.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {165},
numpages = {12},
keywords = {Design Patterns, Human-Robot Interaction, Interface Design, Knowledge Graph},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3706599.3719837,
author = {Ye, Lyumanshan and Jiang, Jiandong and Liu, Yuhan and Ran, Yihan and Chang, Danni},
title = {Colin: A Multimodal Human-AI Co-Creation Storytelling System to Support children’s Multi-Level Narrative Skills},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719837},
doi = {10.1145/3706599.3719837},
abstract = {Children develop narrative skills by understanding and actively building connections between elements, image-text matching, and consequences. However, it is challenging for children to clearly grasp these multi-level links only through explanations of text or facilitator’s speech. To address this, we developed Colin, an interactive storytelling tool that supports children’s multi-level narrative skills through both voice and visual modalities. In the generation stage, Colin supports facilitator to define and review generated text and image content freely. In the understanding stage, a question-feedback model helps children understand multi-level connections while co-creating stories with Colin. In the building phase, Colin actively encourages children to create connections between elements through drawing and speaking. A user study with 20 participants evaluated Colin by measuring children’s engagement, understanding of cause-and-effect relationships, and the quality of their new story creations. Our results demonstrated that Colin significantly enhances the development of children’s narrative skills across multiple levels.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {139},
numpages = {11},
keywords = {Children, AI, Large Language Model, Story-Reading, Interaction, Human-AI Interaction, Personalization, Guided Conversation, Multi-Modal Interaction},
location = {
},
series = {CHI EA '25}
}

@misc{huang2025olympicarenabenchmarkingmultidisciplinecognitive,
      title={OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI}, 
      author={Zhen Huang and Zengzhi Wang and Shijie Xia and Xuefeng Li and Haoyang Zou and Ruijie Xu and Run-Ze Fan and Lyumanshan Ye and Ethan Chern and Yixin Ye and Yikai Zhang and Yuqing Yang and Ting Wu and Binjie Wang and Shichao Sun and Yang Xiao and Yiyuan Li and Fan Zhou and Steffi Chern and Yiwei Qin and Yan Ma and Jiadi Su and Yixiu Liu and Yuxiang Zheng and Shaoting Zhang and Dahua Lin and Yu Qiao and Pengfei Liu},
      year={2025},
      eprint={2406.12753},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12753}, 
}

@misc{zheng2025deepresearcherscalingdeepresearch,
      title={DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments}, 
      author={Yuxiang Zheng and Dayuan Fu and Xiangkun Hu and Xiaojie Cai and Lyumanshan Ye and Pengrui Lu and Pengfei Liu},
      year={2025},
      eprint={2504.03160},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.03160}, 
}
